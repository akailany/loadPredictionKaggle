{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Prediction Kaggle Contest\n",
    "Atiya Kailany <br>\n",
    "May 15, 2021<br>\n",
    "<br>\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:\n",
    "\n",
    "Importing necessary libraies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Loan Prediction Data:\n",
    "\n",
    "The test and train data is being saved into two separate sets of pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./kaggle/input/loan-default-prediction/train_v2.csv\")\n",
    "test_data = pd.read_csv(\"./kaggle/input/loan-default-prediction/test_v2.csv\")\n",
    "\n",
    "knn_train_data = pd.read_csv(\"./kaggle/input/loan-default-prediction/train_v2.csv\")\n",
    "knn_test_data = pd.read_csv(\"./kaggle/input/loan-default-prediction/test_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data for KNN with raw data:\n",
    "\n",
    "Eliminating incompatible types of entries and substituting null and missing values with 0â€™s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trying k nearest neighbor with all data, without removing outliers and missing values\n",
    "\n",
    "\n",
    "# get columns with invalid data type\n",
    "invalid_columns = knn_train_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# remove invalid columns from data sets\n",
    "knn_train_data.drop(invalid_columns, axis=1, inplace=True)\n",
    "knn_test_data.drop(invalid_columns, axis=1, inplace=True)\n",
    "\n",
    "# remove the 'id' column from the test set\n",
    "knn_train_data.drop('id', axis=1, inplace = True)\n",
    "knn_test_data.drop('id', axis=1, inplace = True)\n",
    "\n",
    "\n",
    "# fill in null values with 0's.\n",
    "knn_train_data = knn_train_data.fillna(0)\n",
    "knn_test_data = knn_test_data.fillna(0)\n",
    "\n",
    "\n",
    "all_train_data = knn_train_data\n",
    "train_loss = all_train_data.loc[:,'loss']\n",
    "all_train_data.drop('loss', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Second set of Data for all other models:\n",
    "\n",
    "- Removing invalid columns\n",
    "- substituting null and missing values with mean of X_train\n",
    "- Dropping id columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# get columns with invalid data type\n",
    "invalid_columns = train_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# remove invalid columns from data sets\n",
    "train_data.drop(invalid_columns, axis=1, inplace=True)\n",
    "test_data.drop(invalid_columns, axis=1, inplace=True)\n",
    "\n",
    "# remove the 'id' column from the test set\n",
    "test_data.drop('id', axis=1, inplace = True)\n",
    "\n",
    "# sum data cells with null values\n",
    "null_values = train_data.isnull().sum()\n",
    "#print(null_values)\n",
    "\n",
    "# data frame containing each column with null values\n",
    "null_values = pd.DataFrame(null_values[null_values!=0])\n",
    "#print(null_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Second set of Data for all other models (Continued):\n",
    "\n",
    "In addition, a train-test-split is performed for model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# get features and target for train-test-split\n",
    "features = train_data.iloc[:,1:751].copy()\n",
    "target = train_data.iloc[:,751].copy()\n",
    "\n",
    "# convert to binary\n",
    "target[target>0] = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, stratify = target, random_state=0)\n",
    "\n",
    "# sum of null values \n",
    "# print(X_train.isnull().sum().sum())\n",
    "# print(test_data.isnull().sum().sum())\n",
    "\n",
    "# fill in null values with mean values.\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "test_data = test_data.fillna(X_train.mean())\n",
    "\n",
    "# check again sum of null values \n",
    "# print(X_train.isnull().sum().sum())\n",
    "# print(test_data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#PCA model\n",
    "\n",
    "# fit the data to the training set\n",
    "scalar= StandardScaler()\n",
    "scalar.fit(X_train)\n",
    "\n",
    "# to training model\n",
    "X_train = scalar.transform(X_train)\n",
    "\n",
    "# for prediction\n",
    "X_test = scalar.transform(test_data)\n",
    "\n",
    "# reduce dimensionality \n",
    "pca = PCA(n_components = 200)\n",
    "pca.fit(X_train)\n",
    "\n",
    "# transform data \n",
    "X_train = pca.transform(X_train)\n",
    "X_train = pd.DataFrame(data = X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "X_test = pd.DataFrame(data = X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Regression model\n",
    "log_reg = LogisticRegression(class_weight='balanced',max_iter=400, random_state=1)\n",
    "\n",
    "# fit model with the data\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "# get prediction\n",
    "log_reg_pred = log_reg.predict(X_test)\n",
    "\n",
    "# check prediction output\n",
    "print(log_reg_pred.shape)\n",
    "print(log_reg_pred)\n",
    "\n",
    "# check submission shape matches prediction shape\n",
    "submission = pd.read_csv(\"./kaggle/input/loan-default-prediction/sampleSubmission.csv\")\n",
    "#print(submission['loss'].shape)\n",
    "\n",
    "# see loss count from prediction\n",
    "sns.countplot(log_reg_pred);\n",
    "\n",
    "submission['loss'] = log_reg_pred\n",
    "submission.to_csv(\"log_reg_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Gaussian model\n",
    "naive = GaussianNB()\n",
    "\n",
    "# fit model with the data\n",
    "naive.fit(X_train, y_train)\n",
    "\n",
    "# get prediction\n",
    "naive_pred = naive.predict(X_test)\n",
    "\n",
    "# see loss count from prediction\n",
    "sns.countplot(naive_pred);\n",
    "\n",
    "submission['loss'] = naive_pred\n",
    "submission.to_csv(\"naive_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Random Forest model\n",
    "ran_forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\n",
    "\n",
    "# fit model with the data\n",
    "ran_forest.fit(X_train, y_train)\n",
    "\n",
    "# get prediction\n",
    "ran_forest_pred = ran_forest.predict(X_test)\n",
    "\n",
    "# see loss count from prediction\n",
    "sns.countplot(ran_forest_pred);\n",
    "\n",
    "submission['loss'] = ran_forest_pred\n",
    "submission.to_csv(\"ran_forest_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal K value determination Code:\n",
    "This code will take a very long time to process as it is running the data set multiple times with\n",
    "different k values and calculating the accuracies in order to determine the optimal k value.\n",
    "\n",
    "P.S. This code has been commented out as it is unnecessary to run, since we already determined 8 to\n",
    "yield the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TO TEST MULTIPLE K VALUES, COMMENTED DUE TO LONG PROCESSING TIMES ~12 mins per k\n",
    "#\n",
    "# k_value = range(1,10)\n",
    "# scores = {}\n",
    "# scores_list = []\n",
    "# for k in range(1,10):\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#     knn.fit(x_train, y_train)\n",
    "#     y_pred = knn.predict(x_test)\n",
    "#     scores[k] = metrics.accuracy_score(y_test, y_pred)\n",
    "#     scores_list.append(metrics.accuracy_score(y_test, y_pred))\n",
    "#\n",
    "# plt.plot(k_value, scores_list)\n",
    "# plt.xlabel('K value for KNN')\n",
    "# plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Optimal K value determination Code:\n",
    "Note this is a screenshot of my output when I ran it, the code will take around 10 minutes for each k value, so around 100 minutes.\n",
    "I've provided a screenshot of the results below for convenience:\n",
    "\n",
    "![optimal k](https://user-images.githubusercontent.com/42689178/118394005-5a3a7300-b5ff-11eb-9890-91a484e02dae.jpg)\n",
    "\n",
    "As seen from the above elbow, 8 is an optimal k value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# K Nearest Neighbor WITHOUT outlier removal.\n",
    "# 11 mins running time with gaming PC\n",
    "\n",
    "knn_x_train, knn_x_test, knn_y_train, knn_y_test = train_test_split(all_train_data, train_loss, random_state=1)\n",
    "\n",
    "#model\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "#fit model with data\n",
    "knn.fit(all_train_data, train_loss)\n",
    "\n",
    "#get prediction\n",
    "knn_pred_outlier = knn.predict(knn_test_data)\n",
    "\n",
    "# see loss count from prediction\n",
    "sns.countplot(knn_pred_outlier);\n",
    "\n",
    "submission['loss'] = knn_pred_outlier\n",
    "submission.to_csv(\"KNearest_without_outlier_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# K Nearest Neighbor with outlier removal.\n",
    "\n",
    "#model\n",
    "# k = 8 is optimal as shown in experiment above.\n",
    "# No need to test again here.\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "#fit model with data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#get prediction\n",
    "knn_pred_without_outlier = knn.predict(X_test)\n",
    "\n",
    "# see loss count from prediction\n",
    "sns.countplot(knn_pred_without_outlier);\n",
    "\n",
    "submission['loss'] = knn_pred_without_outlier\n",
    "submission.to_csv(\"KNearest_neighbor_submission.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}